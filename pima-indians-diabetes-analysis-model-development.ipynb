{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Resources\n* https://www.kaggle.com/code/shrutimechlearn/step-by-step-diabetes-classification-knn-detailed\n* https://www.kaggle.com/code/ash316/ml-from-scratch-part-2\n* https://www.kaggle.com/code/pouryaayria/a-complete-ml-pipeline-tutorial-acu-86 (Promising to implement)\n* https://www.kaggle.com/code/vincentlugat/pima-indians-diabetes-eda-prediction-0-906\n* https://www.kaggle.com/code/faysalmiah1721758/pima-indians-diabetes\n* https://www.kaggle.com/code/walidkw/pima-indians-diabetes-ml-model-selection-83 (Promising to implement)","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T08:09:41.608686Z","iopub.execute_input":"2023-09-12T08:09:41.610074Z","iopub.status.idle":"2023-09-12T08:09:41.662495Z","shell.execute_reply.started":"2023-09-12T08:09:41.610004Z","shell.execute_reply":"2023-09-12T08:09:41.661303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:41.664374Z","iopub.execute_input":"2023-09-12T08:09:41.665634Z","iopub.status.idle":"2023-09-12T08:09:42.121997Z","shell.execute_reply.started":"2023-09-12T08:09:41.665599Z","shell.execute_reply":"2023-09-12T08:09:42.120649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction and data analysis \nDescribe the problem being addressed. Provide a detailed characterization of the task dataset in terms of format, volume, quality and bias.\n\n* Problem Description: \n> Start by introducing the problem or research question that your analysis aims to address. Clearly define the scope and objectives of your analysis. Explain why this problem is important or relevant.\n\n* Dataset Description:\n\n> Format: \n>> Describe the format of the dataset. Is it structured (e.g., CSV, Excel) or unstructured (e.g., text, images)? Mention the data types present (e.g., numerical, categorical). \n\n> Volume: \n>> Specify the size of the dataset in terms of the number of records and features (columns). Mention if it's a small, medium, or large dataset.\n\n> Quality: \n>> Discuss the quality of the dataset. Are there missing values, outliers, or data errors? How were these issues handled (e.g., data imputation, outlier removal)?\n\n> Bias: \n>> Address any potential biases in the dataset. Bias can arise from data collection methods, sampling, or other factors. Describe how bias was considered and handled, if applicable.","metadata":{}},{"cell_type":"markdown","source":"## Format:\nThe Pima Indian Diabetics Dataset is in a structured tabular format. The dataset is available on Kaggle in the CSV (Comma-Seperated-Values) file format. The dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. Each data entries or rows in the dataset represents an individual, whereas each columns represents attributes related to the subject health like glucose level, blood pressure, etc. For the features, most of them are integer values with exception to \"BMI\" and \"DiabetesPedigreeFunction\" that are floating-point value.\n\nTALK ABOUT THE OBJECTIVE OF THE DATASET SOMEWHERE?\n\n## Volume:\nThe dataset consisted of 768 instances, which is considerd to be moderate number of subjects. Each instance comes with a set of medical measurements (like including glucose level, blood pressure, and BMI) and a target varialbe called Outcome that indicate whether the subject has diabetic or not. If the value in the Outcome column reads 0, then the person doesn't have diabetics; otherwise, the person with diabetic will have Outcome column reads 1. \n\n## Quality:\nGiven the statistical analysis, a very intriguing question arises. Is it possible for the columns including \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", and \"BMI\" to have minimum value of zero? In this setting, the value of zero doesn't make sense. For example, a person with BMI means a person weight zero kilogram, which is impossible. We speculate that these instances were treated as missing value (replace with zero) or recording error from human mannual data entry. In order to address this problem, we will considered replacing the zero values with either mean or median of the associated column. \n\nWe used boxplot to investigate the outlier in the dataset. Using the IQR (Inter-qaurtile Range) outlier detection approach, the data that shown as dot on the boxplot is considered as outlier. We also put multiple text annotations to indicate the number of outlier for each feature. \n\nHOW TO ADDRESS THE OUTLIER BEFORE TRAINING THE MODEL?\n\n## Bias:\nThe bias may arise from the sampling process, data collection methods, and population representation. For instance, the dataset includes specific group of people, the Pima Indians; therefore, the analysis and model's applicability may not be suitable for other population. Henceforth, the prediction model is generalized enough to apply with other diverse setting. ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:42.123960Z","iopub.execute_input":"2023-09-12T08:09:42.124959Z","iopub.status.idle":"2023-09-12T08:09:42.161179Z","shell.execute_reply.started":"2023-09-12T08:09:42.124919Z","shell.execute_reply":"2023-09-12T08:09:42.159430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:42.165331Z","iopub.execute_input":"2023-09-12T08:09:42.165955Z","iopub.status.idle":"2023-09-12T08:09:42.210778Z","shell.execute_reply.started":"2023-09-12T08:09:42.165910Z","shell.execute_reply":"2023-09-12T08:09:42.209543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To make sure that there are only two quanitiy in the Outcome (y)\ndf['Outcome'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:42.212665Z","iopub.execute_input":"2023-09-12T08:09:42.214932Z","iopub.status.idle":"2023-09-12T08:09:42.226844Z","shell.execute_reply.started":"2023-09-12T08:09:42.214875Z","shell.execute_reply":"2023-09-12T08:09:42.225083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\noutcome_counts = df['Outcome'].value_counts()\n\ntotal_count = outcome_counts.sum()\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(x=[0, 1], height=outcome_counts, color=['blue', 'red'], alpha=0.7)\nplt.title('Frequency of Test Outcome (Positive vs. Negative)')\nplt.xlabel('Outcome')\nplt.ylabel('Frequency')\nplt.xticks([0, 1], ['Negative Tested (0)', 'Positive Tested (1)'], rotation=0)\n\n# Customize the labels inside the bars\nfor i, (count, percentage) in enumerate(zip(outcome_counts, outcome_counts / total_count * 100)):\n    plt.text(i, count / 2, f'{count} subjects\\n({percentage:.2f}%)', ha='center', va='center', \n             fontsize=12, fontweight='bold', color='white')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:42.228958Z","iopub.execute_input":"2023-09-12T08:09:42.229802Z","iopub.status.idle":"2023-09-12T08:09:44.558403Z","shell.execute_reply.started":"2023-09-12T08:09:42.229746Z","shell.execute_reply":"2023-09-12T08:09:44.557181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_columns = len(df.drop(['Outcome'],axis=1).columns)\nnum_rows = (num_columns // 3) + (num_columns % 3)\n\nfig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(10, 6 * num_rows // 2))\naxes = axes.flatten()\n\nfor i, col in enumerate(df.drop(['Outcome'],axis=1).columns):\n    ax = axes[i]\n    boxplot = df[col].plot(kind='box', ax=ax, sharex=False, sharey=False, \n                           meanline=True, showmeans =True, meanprops={'color': 'red', 'linestyle': '--'})\n    ax.set_title(f'{col}')\n    ax.set_xticks([])\n\n    # Calculate the median and mean\n    median, mean = df[col].median(), df[col].mean() \n    \n    # Mean and Median Annotation\n    ax.text(0.62, 0.9, f'Median: {median:.2f}', transform=ax.transAxes, \n            fontsize=9, verticalalignment='top',color='green')\n    ax.text(0.62, 0.8, f'Mean: {mean:.2f}', transform=ax.transAxes, \n            fontsize=9, verticalalignment='top',color='red')\n\n    # Outlier calculation and Annotation\n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n    ax.text(0.62, 0.7, f'Outliers: {len(outliers)}', transform=ax.transAxes, fontsize=9, verticalalignment='top')\n\nfor i in range(num_columns, len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:44.560413Z","iopub.execute_input":"2023-09-12T08:09:44.560786Z","iopub.status.idle":"2023-09-12T08:09:46.590128Z","shell.execute_reply.started":"2023-09-12T08:09:44.560755Z","shell.execute_reply":"2023-09-12T08:09:46.588833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features = len(df.columns) - 1  \nnum_rows,num_cols = 3, 3\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8 * num_rows // 2))\naxes = axes.flatten()\n\nfor i, column in enumerate(df.columns[:-1]):\n    ax = axes[i]\n    # Plot All the data points\n    sns.histplot(data=df, x=column, bins=20, common_norm=False, ax=ax, \n                 legend=False, color='orange', alpha=0.2, edgecolor='none')\n    \n    # Plot the data points seperated by hue (Outcome)\n    sns.histplot(data=df, x=column, hue=\"Outcome\", bins=20, common_norm=False, kde=True,\n                 ax=ax, legend=False, palette={0: 'blue', 1: 'red'}, alpha=0.4, edgecolor='none')\n    \n    mean = df[column].mean()\n    var = df[column].var()\n    skew = df[column].skew()\n    \n    ax.set_title(f'{column}\\n(Mean={mean:.2f}, Var={var:.2f}, Skew={skew:.2f})')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Frequency')\n    \n    # Calculate seperate mean and variance \n    mean_0 = df[df['Outcome'] == 0][column].mean()\n    mean_1 = df[df['Outcome'] == 1][column].mean()\n    var_0 = df[df['Outcome'] == 0][column].std()\n    var_1 = df[df['Outcome'] == 1][column].std()\n\n    ax.annotate(f'Mean(0): {mean_0:.2f}', xy=(0.7125, 0.85), xycoords='axes fraction', fontsize=10, color='blue')\n    ax.annotate(f'Mean(1): {mean_1:.2f}', xy=(0.7125, 0.75), xycoords='axes fraction', fontsize=10, color='red')\n    ax.annotate(f'Std(0): {var_0:.2f}', xy=(0.7125, 0.65), xycoords='axes fraction', fontsize=10, color='blue')\n    ax.annotate(f'Std(1): {var_1:.2f}', xy=(0.7125, 0.55), xycoords='axes fraction', fontsize=10, color='red')\n    \n    ax.axvline(mean, color='black', linestyle='-.', linewidth=1.5)\n    ax.axvline(df[df['Outcome'] == 0][column].mean(), color='blue', linestyle='--', linewidth=1.5)\n    ax.axvline(df[df['Outcome'] == 1][column].mean(), color='red', linestyle='--', linewidth=1.5)\n\nfor i in range(num_features, num_rows * num_cols):\n    fig.delaxes(axes[i])\n\ncustom_legend = [\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10),\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10),\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10)\n]\nlegend_labels = ['All the Data', 'Negative Tested', 'Positive Tested']\nlegend = fig.legend(custom_legend, legend_labels, loc='lower right', bbox_to_anchor=(1.0, 0.0))\nlegend.set_bbox_to_anchor((0.90, 0.15))\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:46.591635Z","iopub.execute_input":"2023-09-12T08:09:46.592132Z","iopub.status.idle":"2023-09-12T08:09:52.367864Z","shell.execute_reply.started":"2023-09-12T08:09:46.592086Z","shell.execute_reply":"2023-09-12T08:09:52.366507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_rows, num_cols = 3, 3\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8 * num_rows // 2))\naxes = axes.flatten()\n\nfor i, column in enumerate(df.columns[:-1]):\n    ax = axes[i]\n\n    # Plot the KDE (Kernel Density Estimation) plot for the entire dataset\n    sns.kdeplot(data=df, x=column, ax=ax, color='orange', legend=False, linewidth=2, alpha=0.6)\n\n    # Plot the KDE plot for data points separated by hue (Outcome)\n    sns.kdeplot(data=df[df['Outcome'] == 0], x=column, ax=ax, color='blue', label='Negative Tested', linewidth=2, alpha=0.6)\n    sns.kdeplot(data=df[df['Outcome'] == 1], x=column, ax=ax, color='red', label='Positive Tested', linewidth=2, alpha=0.6)\n    \n    # From the previous graph\n    mean = df[column].mean()\n    var = df[column].var()\n    skew = df[column].skew()\n    \n    ax.set_title(f'{column}\\n(Mean={mean:.2f}, Var={var:.2f}, Skew={skew:.2f})')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Density')\n    \n    # Calculate seperate mean and variance \n    mean_0 = df[df['Outcome'] == 0][column].mean()\n    mean_1 = df[df['Outcome'] == 1][column].mean()\n    std_0 = df[df['Outcome'] == 0][column].std()\n    std_1 = df[df['Outcome'] == 1][column].std()\n\n    ax.annotate(f'Mean(0): {mean_0:.2f}', xy=(0.7125, 0.85), xycoords='axes fraction', fontsize=10, color='blue')\n    ax.annotate(f'Mean(1): {mean_1:.2f}', xy=(0.7125, 0.75), xycoords='axes fraction', fontsize=10, color='red')\n    ax.annotate(f'Std(0): {std_0:.2f}', xy=(0.7125, 0.65), xycoords='axes fraction', fontsize=10, color='blue')\n    ax.annotate(f'Std(1): {std_1:.2f}', xy=(0.7125, 0.55), xycoords='axes fraction', fontsize=10, color='red')\n\n    \n\n# Remove remaining empty subplots\nfor i in range(num_features, num_rows * num_cols):\n    fig.delaxes(axes[i])\n\nhandles, labels = ax.get_legend_handles_labels()\ncustom_legend = [\n    plt.Line2D([0], [0], color='orange', lw=2, label='All the Data'),\n    *handles \n]\nlegend_labels = ['All the Data', *labels]\nlegend = fig.legend(custom_legend, legend_labels, loc='lower right', bbox_to_anchor=(1.0, 0.0))\nlegend.set_bbox_to_anchor((0.90, 0.15))\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:52.369736Z","iopub.execute_input":"2023-09-12T08:09:52.370112Z","iopub.status.idle":"2023-09-12T08:09:56.135843Z","shell.execute_reply.started":"2023-09-12T08:09:52.370081Z","shell.execute_reply":"2023-09-12T08:09:56.134397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nhmap = sns.heatmap(df.corr(), annot=True, cmap='RdBu', linewidths=3.5, linecolor='white')\n\n# Add a title\nplt.title(\"Pearson Correlation between Features\")\n\nhmap.set_xticklabels(hmap.get_xticklabels(), rotation=90, fontsize=9)\nhmap.set_yticklabels(hmap.get_yticklabels(), fontsize=9)\n\n# Customize the legend\ncbar = hmap.collections[0].colorbar\ncbar.set_label('Correlation', fontsize=10)\ncbar.ax.tick_params(labelsize=10)\n\n# Add additional artistic elements\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:09:56.139777Z","iopub.execute_input":"2023-09-12T08:09:56.140294Z","iopub.status.idle":"2023-09-12T08:09:56.946493Z","shell.execute_reply.started":"2023-09-12T08:09:56.140250Z","shell.execute_reply":"2023-09-12T08:09:56.945091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Manipulation\n* Remove outliers from the dataset \n* Replace zero values in columns that doesn't make sense using mean or median\n* Upsampling to handle data imbalance between Positive and Negative Dataset (ex. SMOT?)","metadata":{}},{"cell_type":"markdown","source":"# Training a Machine Learning and Deep Learning Model\n* Implement fives Algorithm to predict \n* Search model from the resources provided above (not limited to Deep Learning Model)","metadata":{}},{"cell_type":"code","source":"from keras.layers import InputLayer, Conv1D, Dense, Flatten, MaxPooling1D, BatchNormalization, LeakyReLU\nfrom keras.layers.core import Dropout\nfrom keras.models import Sequential\nfrom keras.regularizers import l2","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:21:58.635719Z","iopub.execute_input":"2023-09-12T09:21:58.636190Z","iopub.status.idle":"2023-09-12T09:21:58.643600Z","shell.execute_reply.started":"2023-09-12T09:21:58.636154Z","shell.execute_reply":"2023-09-12T09:21:58.641738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nx = df.drop(columns=['Outcome'],axis=1)\ny = df['Outcome']\n\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\n\nx = x.reshape(x.shape[0], x.shape[1], 1)\ny = y.to_numpy()\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:08:07.121550Z","iopub.execute_input":"2023-09-12T09:08:07.122078Z","iopub.status.idle":"2023-09-12T09:08:07.140660Z","shell.execute_reply.started":"2023-09-12T09:08:07.122025Z","shell.execute_reply":"2023-09-12T09:08:07.139165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Creation\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(8,)))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation= 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:08:09.234495Z","iopub.execute_input":"2023-09-12T09:08:09.234947Z","iopub.status.idle":"2023-09-12T09:08:09.331786Z","shell.execute_reply.started":"2023-09-12T09:08:09.234910Z","shell.execute_reply":"2023-09-12T09:08:09.330249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing Model\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(x_train.shape[1],1)))\n# model.add(Conv1D(filters=8, kernel_size=3, activation='relu'))\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=8, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(16, activation= LeakyReLU(alpha=0.01), \n                kernel_regularizer=l2(0.01)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(8, activation='relu',\n               kernel_regularizer=l2(0.01)))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:36:06.913447Z","iopub.execute_input":"2023-09-12T09:36:06.913834Z","iopub.status.idle":"2023-09-12T09:36:07.085459Z","shell.execute_reply.started":"2023-09-12T09:36:06.913803Z","shell.execute_reply":"2023-09-12T09:36:07.084344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom keras.optimizers import Adam, RMSprop\n\nadam = Adam(learning_rate = 0.001)\nrmsprop = RMSprop(learning_rate = 0.001)\n\nmodel.compile(\n    loss = 'binary_crossentropy',\n    optimizer = adam,\n    metrics = ['accuracy']\n)\n\nhist = model.fit(\n    x_train, y_train,\n    epochs=150,\n    batch_size=10,\n    validation_data=(x_test, y_test),\n    verbose=2\n)\nclear_output()\n\n_, score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score)\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\n\n# Summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:37:52.793872Z","iopub.execute_input":"2023-09-12T09:37:52.795491Z","iopub.status.idle":"2023-09-12T09:38:30.641915Z","shell.execute_reply.started":"2023-09-12T09:37:52.795433Z","shell.execute_reply":"2023-09-12T09:38:30.640568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}